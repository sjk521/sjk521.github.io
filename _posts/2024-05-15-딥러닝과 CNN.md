---
title: "[AI 홀로서기] 2. 딥러닝과 CNN"
date: 2024-05-15  #YYYY-MM-DD HH:MM:SS +/-TTTT
last_modified_at : 2024-05-15
categories: [AI, 기초수업] #[TOP_CATEGORIE, SUB_CATEGORIE]
tags: [ai]     # TAG names should always be lowercase
#math: true

toc : true
toc_sticky : true
---


# 딥러닝의 학습 과정

### 딥러닝

인공지능 > 머신러닝 > 딥러닝 

딥러닝 : 머신러닝의 한 방법으로 사람의 신경 작동을 모방한 학습 방법

사람의 신경 전달 물질인 뉴런을 모방한 ‘퍼셉트론’을 기반으로 복잡한 패턴을 인식하고 예측에 활용 → 딥러닝

6B = 60억개

현업에서도 외부 프로젝트 수주 과정에서 ‘사람이라면 그 문제를 풀 수 있는 지’를 기준으로 프로젝트 가능성 판단

### 3가지 학습의 과정

1. (현재 상태에서 최선의) 결과를 생성 ⇒ Feed Forward
2. 목표와의 차이를 명시 ⇒ Loss
3. 차이를 줄이는 방향으로 개선 ⇒ Optimize

차이가 목적한 수준까지 도달 혹은 정한 학습의 양을 달성할 때까지.

### 학습을 한다

퍼셉트론를 이어주는 가중치(weight)의 적절한 값을 찾아준다

학습의 직접 목표이므로 파라미터 라고도 함

단, 새로운 가중치가 생성되고 없어지는 인공지능 네트워크는 없음

# MLP

### 퍼셉트론의 역할

1. 앞 weight에서 정보 수신
2. 신호 합산 ( + bias)
3. 활성 함수 적용 : 어느 정도로 정보를 보낼 지 결정 $output = f(\Sigma x_iw_i + b)$
    1. bias는 이 뉴런의 기본적으로 어느정도 민감하게 반응해야 할 지
4. 출력

### MLP (Multi Layer Perceptron)

1950년대 

여러 층의 퍼셉트론

층 내에서는 퍼셉트론 연결 X

앞과 뒤 층에 퍼셉트론들은 모두 연결 → fullt connected layer

bias는 input layer에만 없음

모델을 만들 때 정할 수 있는 것 : 

입력과 출력층 외에 hidden layer의 개수와 

각 layer에 존재하는 퍼셉트론의 수

입력 데이터는 vector의 형태

flattening = vectorization 대부분 가능

이미지 (CNN), 시계열 (RNN)의 대표적인 특성을 벗어나는 데이터는 MLP를 많이 사용

구조적 특성 없는 단순 MLP 네트워크는 잘 사용되지 않음

기본 성능의 척도로 활용 됨

Feature의 형태를 정리하거나, feature에서 정보 이전 등의 과정에서 많이 사용

출력의 형태를 맞춰주는 경우 

attention과 같이 feature에서 정보를 추출 & 주입하는 경우

# CNN

사람이 이미지를 인식하는 과정

- 시각적인 전체 정보를 작은 단위로 나누고 분석
- 분석된 정보를 모아서 상위 개념 형성
- 상위 개념들을 더 병합하는 과정을 반복해서 전체적인 이미지 인식

### CNN 모델 학습 과정

- Convolution Filter 적용 : 정보를 추출
- Pooling : 정보를 수집
- Convolution과 Pooling 과정 반복

### Filter

이미지 위를 움직이며 정해진 연산을 수행하는 연산자

작은 단위로 정보를 처리하기 위해 활용

### Convolution Filter (=Kernel)

목적 데이터 위를 움직임

1D 데이터 → 하나의 축을 기준으로

2D 데이터 (이미지) → 두 축을 기준으로

3D 데이터 (영상, 모션) → 세 축을 기준으로

필터의 특성을 결정하는 변수 존재

Filter 크기 (주로 홀수 x 홀수)

Stride : 몇 칸씩 움직이는 지

Padding : 원래의 이미지 테두리에 크기를 추가해서 filter 적용 후에도 원본 이미지 크기가 줄어지지 않게 함

입력이 여러 Channel 로 이루어질 수 있음 (R, G, B)

이처럼 출력도 여러 Channel로 이루어질 수 있으며, 입력과 동일한 개수가 아니어도 됨.

출력되는 Channel의 개수는 필터의 개수와 동일함. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7bab67f3-66f3-4be2-bd85-761e233233c9/23a93953-73a5-4985-a756-0f4af45a1856/Untitled.png)

Convolutional Filter 적용 후 Feature Map 크기

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7bab67f3-66f3-4be2-bd85-761e233233c9/74bf3468-556f-4cb2-9b91-d03f6df0ed64/Untitled.png)

$O = \frac{I+2P-K}{S} + 1$  ,  where $I=$ Input, $P =$  Padding, $S =$  Stride, $K =$  Kernel size

### Pooling

Convolution filter로 만들어진 feature map에서 중요한 정보를 유지하면서 주변의 정보(kernel 단위)를 모아 feature의 크기를 줄이는 과정

- Max Pooling : 학습속도는 느리지만 성능이 높음
- Average Pooling : 학습속도는 빠르지만 성능이 낮음

Kernel을 사용하기 때문에 feature map의 변화가 있음

수식은 동일

보통 kernel은 2x2를 많이 사용 → w와 h가 각각 절반이 됨

  

# LeNet, VGG, ResNet

LeNet 1998

VGG 2014

ResNet 2015

## LeNet

LeNet 얀 르쿤

Convolution Filter & Pooling Filter을 반복 후 MLP로 예측하는 CNN의 근본 틀을 만듦.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7bab67f3-66f3-4be2-bd85-761e233233c9/26723606-aa74-44d8-b967-b6345f3e29e3/Untitled.png)

input : 3 x 32 x 32 일 때

Kernel size, Stride, Padding 찾기

(32 + 2P - K) / S + 1 = 28

S 는 주로 1

P 는 여기서는 0

K = 5 로 유추 가능

Q. 6@28x28은 필터가 6개였다는 의미인것?

## VGG

convolution쓸 때 3x3 짜리 convolutional을 N겹하고, max pooling 하는 조합 소개

이 형태를 VGG Block 이라고 함 (Q. 이 부분 더 공부해보기)

Receptive Field 개념 소개

N 겹의 3X3 Conv가 같은 RF에 대해 효율적임을 보여줌

작은 conv일 지라도 겹쳐서 사용하면 다루는 conv의 영역이 넓어져서 효율적이다 (?)

작은 Conv가 여러번 쓰이는 것의 장점 (Q. 작은 Conv이라는 것이 무슨뜻)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7bab67f3-66f3-4be2-bd85-761e233233c9/732abeb7-b008-484f-980b-ecf963b220a8/Untitled.png)

2번 C → Pooling (빨강) → 2번 C → P → 3 X C → P → 3 C → P → 3C → P → MLP 

첫번째 2번 C에서 224 x 224 x 3을 입력했는데 출력도 224 x 224 x 64 이려면, feature의 크기가 줄어들 지 않도록 padding 조절. 같은 feature 크기에 따라서도 깊이에 차이를 둘 수 있음.

VGG Block의 Layer 수에 따라 5가지 종류의 VGG 존재

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7bab67f3-66f3-4be2-bd85-761e233233c9/4a30a39f-8c9a-4533-a24f-e2a494cff739/Untitled.png)

LRN은 더 이상 안씀 → 이제는 Batch Norm 으로 대체

## ResNet

2015년 ImageNet challenge 우승 (사람보다 error rate 낮음)

현재도 이미지 내 feature 추출하기 위해 다수 활용

Layer마다 잔차만 학습하도록 구조적으로 제시

성능 향상

비대한 모델에 의해 발생하는 gradient vanishing 문제 해결